{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400e61bf-7237-4b26-82b7-a11ca1100ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11224af8-0899-4ae1-90e6-87c9efdc0b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports & NLTK Downloads\n",
    "# Imports\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ded53b1-ef29-4406-8b42-a8430dce3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthetic Dataset Generator\n",
    "LABELS = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "lagos_locations = [\n",
    "    \"Lekki\", \"Ajah\", \"Ikorodu\", \"Surulere\", \"Yaba\", \"Victoria Island\",\n",
    "    \"Ikeja\", \"Agege\", \"Iyana Ipaja\"\n",
    "]\n",
    "\n",
    "pidgin_phrases_negative = [\n",
    "    \"water don full everywhere\",\n",
    "    \"road don block finish\",\n",
    "    \"I dey stranded for\",\n",
    "    \"house don flood\",\n",
    "    \"no motor fit pass\",\n",
    "    \"we need help abeg\",\n",
    "    \"LASEMA never show\",\n",
    "    \"NEMA where una dey\",\n",
    "]\n",
    "\n",
    "english_negative = [\n",
    "    \"My street is completely flooded\",\n",
    "    \"Cars are stuck in the water\",\n",
    "    \"People are trapped in their houses\",\n",
    "    \"No rescue team yet\",\n",
    "    \"The drainage has totally failed\",\n",
    "    \"This flood is getting worse\",\n",
    "]\n",
    "\n",
    "neutral_phrases = [\n",
    "    \"It rained heavily in\",\n",
    "    \"Serious downpour this morning around\",\n",
    "    \"Flood reported in parts of\",\n",
    "    \"Traffic is slow because of water on the road in\",\n",
    "    \"Update: water level rising in\",\n",
    "]\n",
    "\n",
    "positive_phrases = [\n",
    "    \"LASEMA is finally on ground at\",\n",
    "    \"Rescue teams just arrived at\",\n",
    "    \"Water is going down gradually in\",\n",
    "    \"Neighbours are helping each other at\",\n",
    "    \"Government just sent trucks to\",\n",
    "]\n",
    "\n",
    "\n",
    "def generate_synthetic_tweet(label: str) -> str:\n",
    "    loc = random.choice(lagos_locations)\n",
    "    \n",
    "    if label == \"negative\":\n",
    "        if random.random() < 0.5:\n",
    "            base = random.choice(pidgin_phrases_negative)\n",
    "            return f\"{base} for {loc} o. This Lagos flood no be small thing.\"\n",
    "        else:\n",
    "            base = random.choice(english_negative)\n",
    "            return f\"{base} in {loc}. #LagosFlood #flood\"\n",
    "    \n",
    "    elif label == \"neutral\":\n",
    "        base = random.choice(neutral_phrases)\n",
    "        return f\"{base} {loc}. #Lagos #rain\"\n",
    "    \n",
    "    elif label == \"positive\":\n",
    "        base = random.choice(positive_phrases)\n",
    "        return f\"{base} {loc}. Thank God things are improving. ðŸ™\"\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def generate_synthetic_dataset(n_per_class: int = 600) -> pd.DataFrame:\n",
    "    data = []\n",
    "    for label in LABELS:\n",
    "        for _ in range(n_per_class):\n",
    "            txt = generate_synthetic_tweet(label)\n",
    "            data.append({\"text\": txt, \"label\": label})\n",
    "    random.shuffle(data)\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0e97b8-9ef0-4728-b29c-485fd8c85419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing & Pidgin Normalization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pidgin_normalization_dict = {\n",
    "    \"dey\": \"is\",\n",
    "    \"don\": \"already\",\n",
    "    \"go-slow\": \"traffic\",\n",
    "    \"goslow\": \"traffic\",\n",
    "    \"hold up\": \"traffic\",\n",
    "    \"abeg\": \"please\",\n",
    "    \"no be small thing\": \"very serious\",\n",
    "    \"wahala\": \"problem\",\n",
    "    \"una\": \"you all\",\n",
    "    \"o\": \"\",  # often emphasis\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_pidgin(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    for k, v in pidgin_normalization_dict.items():\n",
    "        text = re.sub(rf\"\\b{k}\\b\", v, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Remove URLs, mentions, hashtags\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"@\\w+\", \" \", text)\n",
    "    text = re.sub(r\"#\", \" \", text)\n",
    "\n",
    "    # Lowercase & normalize pidgin\n",
    "    text = normalize_pidgin(text)\n",
    "\n",
    "    # Keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "\n",
    "    # Tokenize, remove stopwords, lemmatize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(t)\n",
    "        for t in tokens\n",
    "        if t not in stop_words and len(t) > 2\n",
    "    ]\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d369a8-050c-4a36-9b42-cb3b5e7feb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label  \\\n",
      "0  Rescue teams just arrived at Victoria Island. ...  positive   \n",
      "1  The drainage has totally failed in Ajah. #Lago...  negative   \n",
      "2  My street is completely flooded in Ajah. #Lago...  negative   \n",
      "3  NEMA where una dey for Ikeja o. This Lagos flo...  negative   \n",
      "4  LASEMA is finally on ground at Surulere. Thank...  positive   \n",
      "\n",
      "                                          clean_text  \n",
      "0  rescue team arrived victoria island thank god ...  \n",
      "1      drainage totally failed ajah lagosflood flood  \n",
      "2    street completely flooded ajah lagosflood flood  \n",
      "3                     nema ikeja lagos flood serious  \n",
      "4  lasema finally ground surulere thank god thing...  \n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "positive    600\n",
      "negative    600\n",
      "neutral     600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Generate Dataset & Inspect\n",
    "df = generate_synthetic_dataset(n_per_class=600)  # total 1800 samples\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "print(df.head())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c28ddf-fd2b-4bf3-a200-1c1e466a5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00       120\n",
      "     neutral       1.00      1.00      1.00       120\n",
      "    positive       1.00      1.00      1.00       120\n",
      "\n",
      "    accuracy                           1.00       360\n",
      "   macro avg       1.00      1.00      1.00       360\n",
      "weighted avg       1.00      1.00      1.00       360\n",
      "\n",
      "=== SVM CONFUSION MATRIX (neg, neu, pos) ===\n",
      "[[120   0   0]\n",
      " [  0 120   0]\n",
      " [  0   0 120]]\n"
     ]
    }
   ],
   "source": [
    "#Train & Evaluate SVM Baseline\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean_text\"],\n",
    "    df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), max_features=10000)),\n",
    "    (\"clf\", LinearSVC()),\n",
    "])\n",
    "\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "\n",
    "print(\"=== SVM CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test, y_pred_svm, labels=LABELS))\n",
    "\n",
    "print(\"=== SVM CONFUSION MATRIX (neg, neu, pos) ===\")\n",
    "print(confusion_matrix(y_test, y_pred_svm, labels=LABELS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbccbb3-2a06-4666-822e-d70ec5b6e4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c634c108d5a4e5fba0b54659730f7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0674d5c7491404b81b1d814cab88199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32d1d6cc8be4faeab6560f3d0c28f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626e1bd0053b4395a758eb545e992753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Prepare Data for Transformer\n",
    "# Use original text for transformer model\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "label2id = {l: i for i, l in enumerate(LABELS)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
    "test_ds = Dataset.from_pandas(test_df[[\"text\", \"label\"]])\n",
    "\n",
    "def add_labels(batch):\n",
    "    return {\"labels\": [label2id[l] for l in batch[\"label\"]]}\n",
    "\n",
    "train_ds = train_ds.map(add_labels, batched=True)\n",
    "test_ds = test_ds.map(add_labels, batched=True)\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"  # later you can swap to CrisisBERT-like model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "test_ds = test_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "train_ds.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "test_ds.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "832ec4ad-0508-463f-ab3a-7dea03377801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_11524\\3351515455.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\USER\\anaconda3\\envs\\crisis-nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 31:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.670900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=270, training_loss=0.07039890529380904, metrics={'train_runtime': 1904.7695, 'train_samples_per_second': 2.268, 'train_steps_per_second': 0.142, 'total_flos': 142081245573120.0, 'train_loss': 0.07039890529380904, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define & Train Transformer Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABELS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_lagos_floods\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False\n",
    ")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\"\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd0ea5c5-400a-4d4e-9b0f-0f999d065c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\crisis-nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRANSFORMER AGGREGATE METRICS ===\n",
      "{'eval_loss': 0.0018760310485959053, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 38.087, 'eval_samples_per_second': 9.452, 'eval_steps_per_second': 0.315, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\crisis-nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSFORMER CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00       120\n",
      "     neutral       1.00      1.00      1.00       120\n",
      "    positive       1.00      1.00      1.00       120\n",
      "\n",
      "    accuracy                           1.00       360\n",
      "   macro avg       1.00      1.00      1.00       360\n",
      "weighted avg       1.00      1.00      1.00       360\n",
      "\n",
      "\n",
      "=== TRANSFORMER CONFUSION MATRIX (neg, neu, pos) ===\n",
      "[[120   0   0]\n",
      " [  0 120   0]\n",
      " [  0   0 120]]\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Transformer Model\n",
    "metrics = trainer.evaluate()\n",
    "print(\"=== TRANSFORMER AGGREGATE METRICS ===\")\n",
    "print(metrics)\n",
    "\n",
    "preds_output = trainer.predict(test_ds)\n",
    "y_true = preds_output.label_ids\n",
    "y_pred = preds_output.predictions.argmax(axis=-1)\n",
    "\n",
    "print(\"\\n=== TRANSFORMER CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(\n",
    "    [id2label[i] for i in y_true],\n",
    "    [id2label[i] for i in y_pred],\n",
    "    labels=LABELS\n",
    "))\n",
    "\n",
    "print(\"\\n=== TRANSFORMER CONFUSION MATRIX (neg, neu, pos) ===\")\n",
    "print(confusion_matrix(\n",
    "    [id2label[i] for i in y_true],\n",
    "    [id2label[i] for i in y_pred],\n",
    "    labels=LABELS\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf3fb62-a5a5-4eac-8c5b-bb84328d27f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved svm_pipeline.pkl\n",
      "Saved transformer model + tokenizer to bert_lagos_model/\n",
      "Saved synthetic_lagos_floods.csv\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 1. Save the SVM pipeline\n",
    "joblib.dump(svm_pipeline, \"svm_pipeline.pkl\")\n",
    "print(\"Saved svm_pipeline.pkl\")\n",
    "\n",
    "# 2. Save the transformer model + tokenizer\n",
    "model_dir = \"./bert_lagos_model\"\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "model.save_pretrained(model_dir)\n",
    "print(\"Saved transformer model + tokenizer to bert_lagos_model/\")\n",
    "\n",
    "# 3. Save the synthetic dataset (optional but useful for dashboard charts)\n",
    "df.to_csv(\"synthetic_lagos_floods.csv\", index=False)\n",
    "print(\"Saved synthetic_lagos_floods.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12e697-c368-46f7-9e85-5c65494b3572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
